{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the necessary packages. We will then create some functions to use in our calculations. Please note that while we did start with creating a function to compute a tfidf score, we do not use it throughout the assignment. We use sklearn's TfidfVectorizer instead as it comes with attributes that allow us to remove common English words as well as rare words. We still did the calculation function for practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries and set graphing theme\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define all functions that we use in this project\n",
    "#To calculate a tf score\n",
    "def tf_score(text):\n",
    "    term_counter = Counter(text)\n",
    "    for term in term_counter.keys():\n",
    "        term_counter[term] = term_counter[term]/float(len(text))\n",
    "    \n",
    "    return term_counter\n",
    "\n",
    "#To calculate an idf score\n",
    "def idf_score(document_list):\n",
    "    document_counter = {}\n",
    "\n",
    "    for document in document_list:\n",
    "        for word in document:\n",
    "            document_counter[word]=0\n",
    "\n",
    "    n = len(document_list)\n",
    "\n",
    "    for document in document_list:\n",
    "        encountered_words = set()\n",
    "        for word, val in document.items():\n",
    "            if val > 0 and word not in encountered_words:\n",
    "                document_counter[word] += 1\n",
    "                encountered_words.add(word)\n",
    "\n",
    "    for word, val in document_counter.items():\n",
    "        document_counter[word] = math.log(n/float(val))\n",
    "    \n",
    "    return document_counter\n",
    "\n",
    "#To calculate a tfidf score\n",
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf_score = {}\n",
    "    for word, val in tf.items():\n",
    "        tf_idf_score[word] = val * idf[word]\n",
    "\n",
    "    return tf_idf_score\n",
    "\n",
    "#To create tfidf dataframe\n",
    "def create_tfidf_dataframe(data, vectorizer):\n",
    "    tfidf_values = vectorizer.transform(data['review'])\n",
    "    tfidf_df = pd.DataFrame(tfidf_values.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    tfidf_df['sentiment'] = data['sentiment'].reset_index(drop=True)\n",
    "    return tfidf_df\n",
    "\n",
    "#To separate features and target\n",
    "def separate_features_and_target(dataframe, target_column='sentiment'):\n",
    "    X = dataframe.drop(target_column, axis=1).values\n",
    "    y = dataframe[target_column].values\n",
    "    return X, y\n",
    "\n",
    "#To calculate the step function\n",
    "def ro(t):\n",
    "    return 1.0/t\n",
    "\n",
    "#To calculate the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return np.clip(scipy.special.expit(x),1e-20,1)\n",
    "\n",
    "def conditional_expectation(x, beta):\n",
    "    return sigmoid(np.dot(x,beta))\n",
    "\n",
    "#To calculate the gradient\n",
    "def gradient(lamb, beta, n, B, y, x, y_hat):\n",
    "    regularization_term = -1/(lamb**2) * beta\n",
    "    \n",
    "    sum_term = np.sum((y - y_hat).reshape(-1, 1) * x, axis=0)\n",
    "    \n",
    "    g = regularization_term + n/B * sum_term\n",
    "    \n",
    "    return g\n",
    "\n",
    "#To check for convergence by comparing the norms of two consecutive betas\n",
    "def convergence_by_norm(beta1, beta2, tolerance):\n",
    "    if np.any(np.isnan(beta1)) or np.any(np.isnan(beta2)):\n",
    "        raise ValueError('Invalid values encountered: ', beta1, beta2)\n",
    "    if np.abs(np.linalg.norm(beta1) - np.linalg.norm(beta2)) < tolerance:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "     \n",
    "#To find beta MAP while adding diagnoistics metrics and using minibatches and data sweeping\n",
    "def beta_map_sweep(X, y, lamb, B, step_function, convergence_test, seed=None, verbose=False):\n",
    "    if X.shape[0] != len(y):\n",
    "        raise ValueError(\"Mismatched dimensions: X has shape {} but y has length {}\".format(X.shape, len(y)))\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    max_iter = 1000000\n",
    "    max_epochs = max_iter // (X.shape[0] // B)\n",
    "    converged = False\n",
    "    tolerance = 1e-6\n",
    "    t = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    diagnostics = {\n",
    "        \"log_likelihood\": [],\n",
    "        \"gradient_magnitude\": [],\n",
    "        \"beta_update_magnitude\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        if converged:\n",
    "            break\n",
    "        \n",
    "        permuted_indices = np.random.permutation(X.shape[0])\n",
    "\n",
    "        for i in range(0, X.shape[0], B):\n",
    "            t+=1\n",
    "            indices = permuted_indices[i: i+B]\n",
    "            x_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            y_hat = conditional_expectation(x_sample, beta)\n",
    "\n",
    "            grad = gradient(lamb, beta, X.shape[0], B, y_sample, x_sample, y_hat)\n",
    "            step = step_function(t)\n",
    "            beta_new = beta + step*grad\n",
    "            beta_update_magnitude = np.linalg.norm(step*grad)\n",
    "\n",
    "            # Append diagnostics\n",
    "            diagnostics[\"log_likelihood\"].append(predictive_log_likelihood(beta, y_sample, x_sample))\n",
    "            diagnostics[\"gradient_magnitude\"].append(np.linalg.norm(grad))\n",
    "            diagnostics[\"beta_update_magnitude\"].append(beta_update_magnitude)\n",
    "\n",
    "            # Check convergence\n",
    "            converged = convergence_test(beta_new, beta, tolerance)\n",
    "            if converged:\n",
    "                break\n",
    "\n",
    "            # Update beta\n",
    "            beta = beta_new\n",
    "\n",
    "            if verbose and t % 1000 == 0:\n",
    "                print(f'Epoch {epoch+1}, Iteration {t}, Beta: {beta}')\n",
    "                \n",
    "        if converged:\n",
    "            break\n",
    "\n",
    "    return beta, t, diagnostics\n",
    "\n",
    "#To predict future values\n",
    "def predict(x, beta):\n",
    "    probabilities =  sigmoid(np.dot(x,beta))\n",
    "    predictions = np.where(probabilities >= 0.5, 1, 0)\n",
    "    return predictions\n",
    "\n",
    "#To calculate predicitive log likelihood\n",
    "def predictive_log_likelihood(beta, y, x_out):\n",
    "    m = len(y)\n",
    "    log_likelihood = np.sum(y * np.log(sigmoid(np.dot(x_out, beta))) + (1 - y) * np.log(sigmoid(-np.dot(x_out, beta))))\n",
    "    return 1/m * log_likelihood\n",
    "\n",
    "#To evalute the model\n",
    "def evaluate_model(beta, X, y):\n",
    "    predictions = predict(X, beta)\n",
    "    \n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    precision = precision_score(y, predictions)\n",
    "    recall = recall_score(y, predictions)\n",
    "    f1 = f1_score(y, predictions)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "#To evalute the model with predictive log likelihood\n",
    "def evaluate_model_with_ll(beta, X_train, y_train, X_test, y_test):\n",
    "    predictions = predict(X_test, beta)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    train_ll = predictive_log_likelihood(beta, y_train, X_train)\n",
    "    test_ll = predictive_log_likelihood(beta, y_test, X_test)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, train_ll, test_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the dataset, initialize the vectorizer, split our data into a training and a testing set. We thun run two for-loops to ensure that both dataframes, the in and out of sample, have the same number of columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read the dataset and replace \"positive\" with 1, and \"negative\" with 0\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df['sentiment'].replace({\"positive\": 1, \"negative\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To split the data into training and testing, and to intialize the vectorizer\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=0.1,\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "vectorizer.fit(train_df['review'])\n",
    "\n",
    "tfidf_train_df = create_tfidf_dataframe(train_df, vectorizer)\n",
    "tfidf_test_df = create_tfidf_dataframe(test_df, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure that the test and train dataframes have the same number of columns\n",
    "extra_columns = set(tfidf_test_df.columns) - set(tfidf_train_df.columns)\n",
    "missing_columns = set(tfidf_train_df.columns) - set(tfidf_test_df.columns)\n",
    "\n",
    "for column in extra_columns:\n",
    "    tfidf_train_df[column] = 0.0\n",
    "\n",
    "for column in missing_columns:\n",
    "    tfidf_test_df[column] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To separate the X and y values in train and test\n",
    "X_train, y_train = separate_features_and_target(tfidf_train_df)\n",
    "X_test, y_test = separate_features_and_target(tfidf_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find the optimal lambda by having lambda take on a range of values, inputting those values into our beta MAP function, calculating the predictive log-likelihood, and returning the one with the highest log-likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the optimal lambda value\n",
    "performances = []\n",
    "lambda_values = np.logspace(0, 4, 20)\n",
    "B = 100\n",
    "for lamb in lambda_values:\n",
    "    beta = beta_map_sweep(X_train, y_train, lamb, B, ro, convergence_by_norm, seed=0)[0]\n",
    "    lambda_pefromance = predictive_log_likelihood(beta, y_test, X_test)\n",
    "    performances.append(lambda_pefromance)\n",
    "\n",
    "optimal_lambda = lambda_values[np.argmax(performances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a few tests and plots to understand the influence of the prior, the relation between number of iterations and predictive log likelihood, most influential words, and the influence of batch size on convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot the influence of the Prior by ploting the predictive log likelihood on lambda, and to find some metrics to evaluate that influence\n",
    "lambdas = [0.1, 1, 10, 100, 1000]\n",
    "log_likelihoods = []\n",
    "\n",
    "metrics = {\n",
    "    \"lambda\": [], \n",
    "    \"accuracy\": [], \n",
    "    \"precision\": [], \n",
    "    \"recall\": [], \n",
    "    \"f1\": [], \n",
    "    \"train_log_likelihood\": [],\n",
    "    \"test_log_likelihood\": []\n",
    "}\n",
    "\n",
    "for lamb in lambdas:\n",
    "    beta, _, _ = beta_map_sweep(X_train, y_train, lamb, 100, ro, convergence_by_norm)\n",
    "    ll = predictive_log_likelihood(beta, y_test, X_test)\n",
    "    log_likelihoods.append(ll)\n",
    "\n",
    "    accuracy, precision, recall, f1, train_ll, test_ll = evaluate_model_with_ll(beta, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    metrics[\"lambda\"].append(lamb)\n",
    "    metrics[\"accuracy\"].append(accuracy)\n",
    "    metrics[\"precision\"].append(precision)\n",
    "    metrics[\"recall\"].append(recall)\n",
    "    metrics[\"f1\"].append(f1)\n",
    "    metrics[\"train_log_likelihood\"].append(train_ll)\n",
    "    metrics[\"test_log_likelihood\"].append(test_ll)\n",
    "    \n",
    "\n",
    "plt.plot(lambdas, log_likelihoods, 'o-')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda (Regularization Strength)')\n",
    "plt.ylabel('Predictive Log Likelihood')\n",
    "plt.title('Influence of the Prior')\n",
    "plt.show()\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot the predictive log likelihood against the number of iterations\n",
    "def beta_map_sweep_with_ll(X, y, lamb, B, step_function, convergence_test, seed=None, verbose=False):\n",
    "    beta, t,_ = beta_map_sweep(X, y, lamb, B, step_function, convergence_test, seed, verbose)\n",
    "    log_likelihoods = [predictive_log_likelihood(beta, y, X) for _ in range(t)]\n",
    "    return beta, log_likelihoods\n",
    "\n",
    "beta, likelihoods = beta_map_sweep_with_ll(X_train, y_train, optimal_lambda, 100, ro, convergence_by_norm)\n",
    "\n",
    "plt.plot(likelihoods)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Predictive Log Likelihood')\n",
    "plt.title('Predictive Log-Likelihood vs Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure that convergence occurs without any issues\n",
    "beta, _, diagnostics = beta_map_sweep(X_train, y_train, optimal_lambda, 100, ro, convergence_by_norm)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(diagnostics[\"log_likelihood\"], color=\"blue\")\n",
    "plt.title(\"Log-Likelihood per Iteration\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(diagnostics[\"gradient_magnitude\"], color=\"green\")\n",
    "plt.title(\"Gradient Magnitude per Iteration\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(diagnostics[\"beta_update_magnitude\"], color=\"red\")\n",
    "plt.title(\"Beta Update Magnitude per Iteration\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot the top influential words and their coefficients\n",
    "beta, _ = beta_map_sweep_with_ll(X_train, y_train, optimal_lambda, 100, ro, convergence_by_norm)\n",
    "top_n = 10\n",
    "sorted_indices = np.argsort(np.abs(beta))[-top_n:]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "plt.barh(range(top_n), beta[sorted_indices], align='center')\n",
    "plt.yticks(range(top_n), [feature_names[i] for i in sorted_indices])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Top Influential Words')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To plot the numbers of iterations against size of batch to get to convergence\n",
    "batch_sizes = [10, 100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000]\n",
    "iterations_to_converge = []\n",
    "\n",
    "for B in batch_sizes:\n",
    "    _, iterations, _ = beta_map_sweep(X_train, y_train, optimal_lambda, B, ro, convergence_by_norm)\n",
    "    iterations_to_converge.append(iterations)\n",
    "\n",
    "plt.plot(batch_sizes, iterations_to_converge, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Iterations to Converge')\n",
    "plt.title('Influence of Batch Size on Convergence')\n",
    "plt.grid(True)\n",
    "plt.xscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
